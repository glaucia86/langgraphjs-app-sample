# ============================================
# CONFIGURAÇÕES OBRIGATÓRIAS DO LLM
# ============================================
# Todas essas variáveis são OBRIGATÓRIAS e devem ser definidas

# Provedor do modelo (openai ou openai/gpt-4o)
MODEL_PROVIDER=openai

# Nome do modelo a ser usado (exemplo: gpt-4o)
MODEL_NAME=gpt-4o

# Temperatura do modelo (0.0 a 1.0)
MODEL_TEMPERATURE=0

# ============================================
# CONFIGURAÇÕES ESPECÍFICAS POR PROVEDOR
# ============================================

# Para usar GitHub Models (quando MODEL_PROVIDER=openai/gpt-4o)
# Estas são obrigatórias apenas para este provedor:
GITHUB_TOKEN=your_github_token_here
GITHUB_MODELS_ENDPOINT=https://models.inference.ai.azure.com

# Para usar OpenAI diretamente (quando MODEL_PROVIDER=openai)
# Esta é obrigatória apenas para este provedor:
OPENAI_API_KEY=your_openai_api_key_here

# ============================================
# OUTRAS CONFIGURAÇÕES
# ============================================
PORT=3000
NODE_ENV=development

# Configurações de Logging
LOG_LEVEL=info